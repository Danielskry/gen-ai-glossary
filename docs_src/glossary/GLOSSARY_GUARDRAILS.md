# Guardrails

Auto-generated from `data/terms.json`. Do not edit this file manually.

<section class="term-card" data-term="hallucination">

<h3 id="hallucination"><a class="term-anchor" href="#hallucination">Hallucination</a></h3>

<blockquote>
<p>AI hallucinations occur when a model generates content that is factually incorrect, logically flawed, or entirely fabricated while presenting it as confident and accurate. These errors stem from limitations in training data, model architecture, bias, faulty grounding, or the probabilistic way large language models generate outputs. Hallucinations can appear as wrong facts, flawed reasoning, or invented details such as nonexistent studies, citations, or events. They influence everything from chat interactions to computer vision systems, and they can undermine trust, harm decision making, and introduce quality and safety risks in fields like healthcare, law, finance, and software development. Reducing hallucinations requires improved data quality, clear model constraints, retrieval augmented grounding, rigorous testing such as red teaming and adversarial evaluation, and ongoing human oversight.</p>
</blockquote>

<p class="term-meta"><em>Also known as:</em> AI hallucination, model hallucination, LLM hallucination, generative AI hallucination</p>
<p class="term-meta"><em>Sources:</em></p>
<ul class="term-sources">
  <li><a href="https://www.ibm.com/think/topics/ai-hallucinations">https://www.ibm.com/think/topics/ai-hallucinations</a></li>
  <li><a href="https://cloud.google.com/discover/what-are-ai-hallucinations">https://cloud.google.com/discover/what-are-ai-hallucinations</a></li>
  <li><a href="https://www.computer.org/publications/tech-news/trends/hallucinations-in-ai-models">https://www.computer.org/publications/tech-news/trends/hallucinations-in-ai-models</a></li>
</ul>

</section>
