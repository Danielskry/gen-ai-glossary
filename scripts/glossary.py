from __future__ import annotations

import argparse
import html
import json
import re
import sys
from collections import Counter, defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable


KNOWN_CATEGORY_TO_FILE = {
    "agents": "GLOSSARY_AGENTS.md",
    "orchestration": "GLOSSARY_ORCHESTRATION.md",
    "retrieval": "GLOSSARY_RETRIEVAL.md",
    "context-engineering": "GLOSSARY_CONTEXT_ENGINEERING.md",
    "memory": "GLOSSARY_MEMORY.md",
    "guardrails": "GLOSSARY_GUARDRAILS.md",
}

CATEGORY_ALIASES = {
    "memory-optimization": "memory",
    "evaluation-guardrails": "guardrails",
}

NAV_BEGIN = "# BEGIN AUTO-GENERATED GLOSSARY NAV"
NAV_END = "# END AUTO-GENERATED GLOSSARY NAV"
NAV_INDENT = "  "

AUTO_NOTICE = "Auto-generated from `data/terms.json`. Do not edit this file manually."

MARKDOWN_HEADING_PATTERN = re.compile(r"^###\s+(.+)$", re.MULTILINE)
HTML_HEADING_PATTERN = re.compile(r"<h3[^>]*>(.*?)</h3>", re.IGNORECASE | re.DOTALL)


@dataclass
class Term:
    term: str
    definition: str
    category: str
    aliases: list[str]
    sources: list[str]


def default_root() -> Path:
    return Path(__file__).resolve().parents[1]


def load_terms(path: Path) -> list[Term]:
    data = json.loads(path.read_text(encoding="utf-8"))
    if not isinstance(data, list):
        raise ValueError("terms.json must be a JSON array")

    def to_list(value: object) -> list[str]:
        if value is None:
            return []
        if isinstance(value, list):
            return [str(v) for v in value]
        return [str(value)]

    terms: list[Term] = []
    for entry in data:
        if not isinstance(entry, dict):
            continue
        category = str(entry.get("category", "misc")).strip().casefold()
        category = CATEGORY_ALIASES.get(category, category)
        terms.append(
            Term(
                term=str(entry.get("term", "")).strip(),
                definition=str(entry.get("definition", "")).strip(),
                category=category,
                aliases=to_list(entry.get("aliases")),
                sources=to_list(entry.get("sources")),
            )
        )
    return terms


def category_to_filename(category: str) -> str:
    if category in KNOWN_CATEGORY_TO_FILE:
        return KNOWN_CATEGORY_TO_FILE[category]
    slug = re.sub(r"[^A-Za-z0-9]+", "_", category).strip("_")
    return f"GLOSSARY_{slug.upper()}.md"


def friendly_category_title(category: str) -> str:
    if category in KNOWN_CATEGORY_TO_FILE:
        mapping = {
            "agents": "Agent Concepts",
            "orchestration": "Orchestration and Tool Use",
            "retrieval": "Retrieval and RAG Patterns",
            "context-engineering": "Context Engineering",
            "memory": "Memory",
            "guardrails": "Guardrails and Evaluation",
        }
        return mapping.get(category, category.title())
    return re.sub(r"[-_]+", " ", category).title()


def render_markdown(category: str, terms: list[Term]) -> str:
    title = friendly_category_title(category)
    lines = [f"# {title}", "", AUTO_NOTICE, ""]

    def slugify(value: str) -> str:
        slug = re.sub(r"[^a-z0-9]+", "-", value.casefold()).strip("-")
        return slug or "term"

    for term in sorted(terms, key=lambda t: t.term.casefold()):
        if not term.term:
            continue
        slug = slugify(term.term)
        lines.append(f'<section class="term-card" data-term="{slug}">')
        lines.append("")
        heading = html.escape(term.term)
        lines.append(f'<h3 id="{slug}"><a class="term-anchor" href="#{slug}">{heading}</a></h3>')
        lines.append("")
        if term.definition:
            lines.append("<blockquote>")
            paragraphs = [p.strip() for p in term.definition.split("\n\n") if p.strip()]
            for para in paragraphs:
                lines.append(f"<p>{html.escape(para)}</p>")
            lines.append("</blockquote>")
            lines.append("")
        aliases = [a.strip() for a in term.aliases if a and a.strip()]
        if aliases:
            aliases_text = ", ".join(html.escape(a) for a in aliases)
            lines.append(f'<p class="term-meta"><em>Also known as:</em> {aliases_text}</p>')
        sources = [s.strip() for s in term.sources if s and str(s).strip()]
        if sources:
            lines.append('<p class="term-meta"><em>Sources:</em></p>')
            lines.append('<ul class="term-sources">')
            for src in sources:
                safe = html.escape(src)
                if src.startswith(("http://", "https://")):
                    lines.append(f'  <li><a href="{src}">{safe}</a></li>')
                else:
                    lines.append(f"  <li>{safe}</li>")
            lines.append("</ul>")
            lines.append("")
        lines.append("</section>")
        lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def write_files(glossary_dir: Path, grouped: dict[str, list[Term]], dry_run: bool) -> tuple[list[str], list[str]]:
    written: list[str] = []
    removed: list[str] = []
    glossary_dir.mkdir(parents=True, exist_ok=True)
    keep: set[str] = set()

    for category, terms in sorted(grouped.items()):
        filename = category_to_filename(category)
        keep.add(filename)
        content = render_markdown(category, terms)
        path = glossary_dir / filename
        if dry_run:
            written.append(str(path))
            continue
        path.write_text(content, encoding="utf-8")
        written.append(str(path))

    if not dry_run:
        for stale in glossary_dir.glob("GLOSSARY_*.md"):
            if stale.name in keep:
                continue
            stale.unlink(missing_ok=True)
            removed.append(str(stale))

    return written, removed


def slug_from_filename(path: Path) -> str:
    stem = path.stem
    if stem.startswith("GLOSSARY_"):
        stem = stem[len("GLOSSARY_") :]
    return stem.replace("_", "-").casefold()


def glossary_nav_entries(glossary_dir: Path) -> list[tuple[str, str]]:
    entries: list[tuple[str, str]] = []
    for md_file in sorted(glossary_dir.glob("GLOSSARY_*.md")):
        slug = slug_from_filename(md_file)
        title = friendly_category_title(slug)
        rel_path = Path("glossary") / md_file.name
        entries.append((title, rel_path.as_posix()))
    return entries


def render_nav_block(entries: Iterable[tuple[str, str]]) -> list[str]:
    block = [f"{NAV_INDENT}{NAV_BEGIN}"]
    for title, rel_path in entries:
        block.append(f"{NAV_INDENT}- {title}: {rel_path}")
    block.append(f"{NAV_INDENT}{NAV_END}")
    return block


def update_mkdocs_nav(mkdocs_path: Path, glossary_dir: Path) -> None:
    if not mkdocs_path.is_file():
        return
    entries = glossary_nav_entries(glossary_dir)
    block = render_nav_block(entries)
    lines = mkdocs_path.read_text(encoding="utf-8").splitlines()
    try:
        begin = next(i for i, line in enumerate(lines) if NAV_BEGIN in line)
        end = next(i for i, line in enumerate(lines) if NAV_END in line)
    except StopIteration:
        try:
            nav_idx = next(i for i, line in enumerate(lines) if line.strip().startswith("nav:"))
        except StopIteration:
            return
        insert_idx = nav_idx + 1
        while insert_idx < len(lines) and lines[insert_idx].startswith("  "):
            insert_idx += 1
        updated = lines[:insert_idx] + block + lines[insert_idx:]
        mkdocs_path.write_text("\n".join(updated) + "\n", encoding="utf-8")
        return

    updated = lines[:begin] + block + lines[end + 1 :]
    mkdocs_path.write_text("\n".join(updated) + "\n", encoding="utf-8")


def find_duplicate_terms(raw_terms: Iterable[dict]) -> list[str]:
    normalized = [str(t.get("term", "")).casefold() for t in raw_terms]
    counts = Counter(normalized)
    duplicates = []
    for term, count in counts.items():
        if term and count > 1:
            duplicates.append(f'"{term}" appears {count} times')
    return duplicates


def iter_headings(text: str) -> Iterable[str]:
    for heading in MARKDOWN_HEADING_PATTERN.findall(text):
        yield heading
    for raw in HTML_HEADING_PATTERN.findall(text):
        cleaned = re.sub(r"<.*?>", "", raw).strip()
        yield html.unescape(cleaned)


def validate_headings(glossary_dir: Path) -> list[str]:
    issues: list[str] = []
    if not glossary_dir.is_dir():
        return [f"Missing glossary directory: {glossary_dir}"]
    for md in sorted(glossary_dir.glob("GLOSSARY_*.md")):
        text = md.read_text(encoding="utf-8")
        if AUTO_NOTICE not in text:
            continue
        for heading in iter_headings(text):
            if not heading.strip() or "\ufffd" in heading:
                issues.append(f"{md.name}: invalid heading '{heading or '<empty>'}'")
    return issues


def validate_schema(terms: list[dict], schema_path: Path) -> list[str]:
    if not schema_path.is_file():
        return []
    try:
        import jsonschema  # type: ignore
    except ModuleNotFoundError:
        return []

    try:
        schema = json.loads(schema_path.read_text(encoding="utf-8"))
        jsonschema.validate(terms, schema)
    except Exception as exc:  # noqa: BLE001
        return [f"{schema_path.name}: {exc}"]
    return []


def run_generate(args: argparse.Namespace) -> int:
    root = args.root or default_root()
    terms_path = args.terms or (root / "data" / "terms.json")
    glossary_dir = args.glossary_dir or (root / "docs_src" / "glossary")
    mkdocs_path = args.mkdocs or (root / "mkdocs.yml")

    terms = load_terms(terms_path)
    grouped: defaultdict[str, list[Term]] = defaultdict(list)
    for term in terms:
        grouped[term.category].append(term)

    written, removed = write_files(glossary_dir, grouped, args.dry_run)
    if not args.dry_run:
        update_mkdocs_nav(mkdocs_path, glossary_dir)

    prefix = "Would write" if args.dry_run else "Wrote"
    print(f"{prefix}:")
    for path in written:
        print(f" - {path}")
    if removed:
        print("Removed:")
        for path in removed:
            print(f" - {path}")
    return 0


def run_validate(args: argparse.Namespace) -> int:
    root = args.root or default_root()
    terms_path = args.terms or (root / "data" / "terms.json")
    glossary_dir = args.glossary_dir or (root / "docs_src" / "glossary")
    schema_path = args.schema or (root / "data" / "terms.schema.json")

    if not terms_path.is_file():
        print(f"Missing terms file: {terms_path}", file=sys.stderr)
        return 1

    try:
        with terms_path.open(encoding="utf-8") as fh:
            raw_terms = json.load(fh)
    except Exception as exc:  # noqa: BLE001
        print(f"Failed to load {terms_path}: {exc}", file=sys.stderr)
        return 1

    issues: list[str] = []
    issues.extend(find_duplicate_terms(raw_terms))
    issues.extend(validate_schema(raw_terms, schema_path))
    issues.extend(validate_headings(glossary_dir))

    if issues:
        print("Validation issues:", file=sys.stderr)
        for issue in issues:
            print(f" - {issue}", file=sys.stderr)
        return 1

    print("OK")
    return 0


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Generate or validate glossary files.")
    sub = parser.add_subparsers(dest="command", required=True)

    gen = sub.add_parser("generate", help="Generate Markdown files and nav entries.")
    gen.add_argument("--root", type=Path, default=None, help="Repository root (defaults to project root)")
    gen.add_argument("--terms", type=Path, default=None, help="Path to terms.json")
    gen.add_argument("--glossary-dir", type=Path, default=None, help="Directory for generated Markdown files")
    gen.add_argument("--mkdocs", type=Path, default=None, help="Path to mkdocs.yml")
    gen.add_argument("--dry-run", action="store_true", help="Show which files would change without writing")
    gen.set_defaults(func=run_generate)

    val = sub.add_parser("validate", help="Validate data and generated Markdown.")
    val.add_argument("--root", type=Path, default=None, help="Repository root (defaults to project root)")
    val.add_argument("--terms", type=Path, default=None, help="Path to terms.json")
    val.add_argument("--glossary-dir", type=Path, default=None, help="Directory containing generated Markdown")
    val.add_argument("--schema", type=Path, default=None, help="JSON schema describing terms")
    val.set_defaults(func=run_validate)

    return parser


def main(argv: list[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    return args.func(args)


if __name__ == "__main__":
    raise SystemExit(main())
